{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351f2c51",
   "metadata": {},
   "source": [
    "Import all the base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034a38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert, butter, filtfilt, find_peaks, welch\n",
    "from scipy.ndimage import generic_filter\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6d134",
   "metadata": {},
   "source": [
    "## Step 1: Use Bartholomew's distance to identify potential task switching points.\n",
    "\n",
    "Calculate whether the Bhattacharyya distance between 500 milliseconds before and after a given time point exceeds the average of the previous 10 seconds plus 3SD, using 40-millisecond intervals.  \n",
    "Save the time point into the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299134d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bhattacharyya_distance(psd1, psd2):\n",
    "    \"\"\"\n",
    "    Calculate the Bhattacharyya distance between two discrete distributions (power spectra).\n",
    "    psd1 and psd2 should be two power spectral density arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization to probability distribution\n",
    "    p = psd1 / (np.sum(psd1) + 1e-10)\n",
    "    q = psd2 / (np.sum(psd2) + 1e-10)\n",
    "    #bc = 1 mean totally same, bc = 0 mean totally different\n",
    "    bc = np.sum(np.sqrt(p * q))\n",
    "    #bhattacharyya distance=-ln(bc)\n",
    "    return -np.log(bc + 1e-10)\n",
    "\n",
    "\n",
    "#Traverse the entire dataset, save the bd_scores with time points\n",
    "\n",
    "def worker(i, eeg_data_numpy, window_len, sfreq):\n",
    "    front_window = eeg_data_numpy[:, i-window_len:i]\n",
    "    back_window = eeg_data_numpy[:, i:i+window_len]\n",
    "    \n",
    "    _, psd_front = welch(front_window, fs=sfreq, nperseg=window_len)\n",
    "    _, psd_back = welch(back_window, fs=sfreq, nperseg=window_len)\n",
    "    \n",
    "    avg_psd_front = np.mean(psd_front, axis=0)\n",
    "    avg_psd_back = np.mean(psd_back, axis=0)\n",
    "    \n",
    "    return bhattacharyya_distance(avg_psd_front, avg_psd_back)\n",
    "\n",
    "\n",
    "def detect_task_switch_by_bhattacharyya_with_better_CPU(eeg_data_frame, sfreq=256):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        eeg_data_frame (_type_): pandas dataframe (Row: Time point, Column: EEG channel)\n",
    "        gfp (_type_): numpy array of every time point's gfp\n",
    "        sfreq (int, optional): 256hz. Defaults to 256.\n",
    "    return:\n",
    "        A list of time points that may be task switch time point\n",
    "    \"\"\"\n",
    "    entire_time_len = len(eeg_data_frame)\n",
    "    \n",
    "    #use 40ms as the step length of window\n",
    "    step_len = int(0.040 * sfreq)\n",
    "    #use 500 ms as the window length\n",
    "    window_len = int(0.5 * sfreq)\n",
    "    \n",
    "    #Use 10s as the baseline length as filter\n",
    "    #Later use 10s (mean + 3SD) to filt\n",
    "    baseline_len = int(10 * sfreq)\n",
    "    \n",
    "    \n",
    "    #Trans pandas dataframe to numpy array to calculate quicker\n",
    "    #Filp to row is channels, column is time points\n",
    "    eeg_data_numpy = eeg_data_frame.values.T #Now the shape is (channel, time)\n",
    "    \n",
    "    #Do not use for loop anymore. \n",
    "    task_indices = range(window_len, entire_time_len - window_len, step_len)\n",
    "    \n",
    "    bd_results = Parallel(n_jobs=-1)(\n",
    "        delayed(worker)(i, eeg_data_numpy, window_len, sfreq) \n",
    "        for i in task_indices\n",
    "    )\n",
    "    \n",
    "    df_results = pd.DataFrame({\n",
    "        'time_point': list(task_indices),\n",
    "        'bd': bd_results\n",
    "    })\n",
    "    \n",
    "    window_count = baseline_len // step_len\n",
    "    \n",
    "    \n",
    "    df_results['prev_mean'] = df_results['bd'].rolling(window=window_count).mean().shift(1)\n",
    "    df_results['prev_std'] = df_results['bd'].rolling(window=window_count).std().shift(1)\n",
    "    df_results['threshold'] = df_results['prev_mean'] + 3 * df_results['prev_std']\n",
    "\n",
    "    # 过滤：1. 时间大于10s 2. bd值超过阈值\n",
    "    mask = (df_results['time_point'] >= baseline_len) & (df_results['bd'] > df_results['threshold'])\n",
    "    candidate_time_points = df_results.loc[mask, 'time_point'].tolist()\n",
    "    \n",
    "    return candidate_time_points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776cf49c",
   "metadata": {},
   "source": [
    "## Step 2: Alpha and theta check\n",
    "Check 800 ms before and after the time points from step 1. Check average power of envelope for alpha and theta. If average of aplha increase/decrease more than 30%, theta increase/decrease more than 20% and increase and decrease are opposite. The it detect as an task switch. (In compare 200ms before and after time points) Then return the change range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c3c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the average power for a specific frequency(alpha/theta) band using the Welch method.\n",
    "def get_multi_channel_band_power(eeg_data, sfreq, band):\n",
    "\n",
    "    f, psd = welch(eeg_data, fs=sfreq, nperseg=eeg_data.shape[1], axis=-1)\n",
    "    idx = np.logical_and(f >= band[0], f <= band[1])\n",
    "    band_psd = psd[:, idx]\n",
    "    return np.mean(band_psd)\n",
    "\n",
    "#Get the envelope of alpha and theta band\n",
    "def butter_bandpass_filter(data, low, high, sfreq=256, order=4):\n",
    "    nyq = 0.5 * sfreq\n",
    "    low_cut = low / nyq\n",
    "    high_cut = high / nyq\n",
    "    b, a = butter(order, [low_cut, high_cut], btype='band')\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "#Calculate the envelope of each channel.\n",
    "def get_combined_envelope_sq(df, low, high, fs):\n",
    "        data = df.values.T # (channels, time)\n",
    "        \n",
    "        # Bandpass filtering is performed on each channel.\n",
    "        filtered = butter_bandpass_filter(data.T, low, high, sfreq=fs).T\n",
    "        \n",
    "        # Calculate the Hilbert envelope for each channel.\n",
    "        envelopes = np.abs(hilbert(filtered))\n",
    "        \n",
    "        #Since power is V^2 and envelope is V, we need to square the envelope if we want to use the same standard value in paper.\n",
    "        # Take the average along the channel dimension, then square it to obtain the energy trend.\n",
    "        return np.mean(envelopes, axis=0)**2\n",
    "\n",
    "\n",
    "def verify_alpha_theta_2(eeg_data_frame, candidate_time_points, sfreq=256):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        candidate_time_points (list): A list of possible task switch times point\n",
    "        eeg_data_frame: A pandas dataframe of EEG data\n",
    "        sfreq (int): Sampling frequency of the EEG data(256hz)\n",
    "    \"\"\"\n",
    "    \n",
    "    entire_time_len = len(eeg_data_frame)\n",
    "    \n",
    "    \n",
    "    #First get alpha and theta envelope square\n",
    "    alpha_envelope_sq = get_combined_envelope_sq(eeg_data_frame, 8, 13, sfreq)\n",
    "    theta_envelope_sq = get_combined_envelope_sq(eeg_data_frame, 4, 8, sfreq)\n",
    "    \n",
    "    #Set list save verified time points\n",
    "    verified_segments = []\n",
    "    #The window size is 200ms\n",
    "    check_window = int(0.2 * sfreq)\n",
    "    #The range is 800ms before and after the candidate time point\n",
    "    half_range = int(0.8 * sfreq)\n",
    "    #use 40ms as the step length of window\n",
    "    step_len = int(0.040 * sfreq)\n",
    "    \n",
    "    for time_point in candidate_time_points:\n",
    "        search_start = max(0, time_point - half_range)\n",
    "        search_end = min(entire_time_len, time_point + half_range)\n",
    "        \n",
    "        #Set the variable\n",
    "        coarse_start = None\n",
    "        \n",
    "        for time in range(search_start, search_end - check_window, step_len):\n",
    "            \n",
    "            alpha_pre = np.mean(alpha_envelope_sq[time : time+check_window])\n",
    "            alpha_post = np.mean(alpha_envelope_sq[time+check_window : time+2*check_window])\n",
    "            theta_pre = np.mean(theta_envelope_sq[time : time+check_window])\n",
    "            theta_post = np.mean(theta_envelope_sq[time+check_window : time+2*check_window])\n",
    "            \n",
    "            alpha_ratio = (alpha_post - alpha_pre) / (alpha_pre + 1e-10)\n",
    "            theta_ratio = (theta_post - theta_pre) / (theta_pre + 1e-10)\n",
    "            \n",
    "            if abs(alpha_ratio) >= 0.3 and abs(theta_ratio) >= 0.2:\n",
    "                if ((alpha_ratio * theta_ratio) < 0):\n",
    "                    coarse_start = time+check_window\n",
    "                    \n",
    "            \n",
    "            #If found the coarse start. We want to find more details of when the task swich start and end\n",
    "            #Try to find the peak of alpha and theta gradient\n",
    "            if coarse_start is not None:\n",
    "                #return this 200 range of alpha and theta change range\n",
    "                verified_segments.append((time, time+check_window))\n",
    "            \n",
    "                break \n",
    "                # #Set search range be 400 ms for when alpha and theta gradient peak\n",
    "                # search_start = max(0, coarse_start - int(0.4 * sfreq))\n",
    "                # search_end = min(entire_time_len, coarse_start + int(0.4 * sfreq))\n",
    "                \n",
    "                # alpha_grad = np.abs(np.diff(np.sqrt(alpha_envelope_sq[search_start: search_end])))\n",
    "                # theta_grad = np.abs(np.diff(np.sqrt(theta_envelope_sq[search_start: search_end])))\n",
    "                \n",
    "                # if len(alpha_grad) > 0 and len(theta_grad) > 0:\n",
    "                #     precise_alpha = np.argmax(alpha_grad) + search_start\n",
    "                #     precise_theta = np.argmax(theta_grad) + search_start\n",
    "                #     verified_segments.append((min(precise_alpha,precise_theta), max(precise_alpha,precise_theta)))\n",
    "    return verified_segments\n",
    "            \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f16cd3",
   "metadata": {},
   "source": [
    "## Step 3:Check by GFP\n",
    "\n",
    "3 Part in this function:  \n",
    "Part A: There exist lowest GFP in the range of (300ms before task switch start) to (100ms after task switch end) compare to other time. The reason behaind is before task switch brand will shut down most part of brain. The range base on alpha and theta not 100% accurate, so use more time to make sure. Therefore check the lowest GFP in the range of (1500ms before task switch start) to (300ms after task switch end) also in (300ms before task switch start) to (100ms after task switch end).  \n",
    "\n",
    "Part B: Check the mean of lowest GFP in the range of (300ms before task switch start) to (task switch end). The GFP decrease before task switch start and increase after task switch end.  So the mean of 1500ms before task switch start to 300ms should be less than then mean of (300ms before task switch start)'s GFP\n",
    "\n",
    "Part C: From paper, before the task switch, there will be a 20hz to 50hz GFP decrease trend,so try to find the 6 decrease(23ms in 256hz) trend exist in (300ms before task switch start) to (task switch end).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the GFP (Global Field Power) of an EEG signal.\n",
    "def calculate_GFP(eeg_data_frame, sfreq=256):\n",
    "    \"\"\"\n",
    "        Calculate the Global Field Power (GFP) of an EEG data.\n",
    "        Assume data structure is Pandas DataFrame with columns as channels and rows as time points.\n",
    "    \"\"\"\n",
    "    #GFP is the SD of every time point across all channels(column)\n",
    "    #Count and trans to numpy array\n",
    "    gfp = eeg_data_frame.std(axis=1).values\n",
    "    \n",
    "    #Use 30ms as the smoothing range\n",
    "    smooth_range = int(0.03 * sfreq)\n",
    "    # Simple moving about 43ms smoothing removes extremely high frequency spikes\n",
    "    #WHY: \n",
    "        #Assume noise can't 100% remove, so noise can make spikes happen.\n",
    "        #Make spikes more obvious by averaging them out.\n",
    "    smooth_gfp = np.convolve(gfp, np.ones(smooth_range)/smooth_range, mode='same')\n",
    "    \n",
    "    return smooth_gfp\n",
    "\n",
    "def gfp_check(candidate_task_switch, smooth_gfp, sfreq=256):\n",
    "    #Set variable\n",
    "    verified_segments = []\n",
    "    \n",
    "    #Set task switch range before and after. 300ms before start time and 100ms after end time.\n",
    "    before_check_range = int(0.3 * sfreq)\n",
    "    after_check_range = int(0.1 * sfreq)\n",
    "    \n",
    "    \n",
    "#Can change later after discussing \n",
    "    #The check lowest GFP range. 1500 ms before task switch until 300ms after task switch end.\n",
    "        #For alpha and theta, it may change in 200ms. But consider the task swich is happened in undreds to thousands of milliseconds,\n",
    "            #So we set the check range to 1500ms before and 300ms later.\n",
    "    check_lowest_range_before = int(1.5*sfreq)\n",
    "    check_lowest_range_after = int(0.3*sfreq)\n",
    "    \n",
    "    #Check average GFP in the check range is decrease(350hs)\n",
    "    pre_avg_smp = int(0.350 * sfreq)\n",
    "    \n",
    "    #use 25ms as the minumum decrease trend scope\n",
    "    decrease_scope = int(0.025 * sfreq)\n",
    "    \n",
    "    \n",
    "    for start, end in candidate_task_switch:\n",
    "        #Assume large task switch not happen in the first 1500ms\n",
    "        if start < check_lowest_range_before:\n",
    "            continue\n",
    "        \n",
    "        #Part A:\n",
    "        #Set the actuall range of evaluation lowest GFP\n",
    "        lowest_GFP_check_start = start - check_lowest_range_before\n",
    "        lowest_GFP_check_end = min(len(smooth_gfp),end + check_lowest_range_after)\n",
    "\n",
    "        #Check the lowest GFP in the range\n",
    "        gfp_check_data = smooth_gfp[lowest_GFP_check_start : lowest_GFP_check_end]\n",
    "        #Find the lowest GFP in the range\n",
    "        lowest_gfp = np.argmin(gfp_check_data) + lowest_GFP_check_start\n",
    "        \n",
    "        \n",
    "        start_check_point = start - before_check_range\n",
    "        end_check_point = min(len(smooth_gfp), end + after_check_range)\n",
    "        #Check the GFP lowest point of (1500ms+start) to (300ms+end) is also in (300ms+start) to (100ms+end)\n",
    "        is_lowest_in_task_switch = (start_check_point <= lowest_gfp <= end_check_point)\n",
    "        \n",
    "        #If the lowest GFP is not in the task switch range, continue to next candidate\n",
    "        if not is_lowest_in_task_switch:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        #Part B:\n",
    "        #Then, check the average GFP near to start are less than far way to start.\n",
    "        #In this function, compare 1500ms to 350ms before start time and 350ms before start time\n",
    "        average_futher_GFP = np.mean(smooth_gfp[start - check_lowest_range_before : start - pre_avg_smp])\n",
    "        average_near_GFP = np.mean(smooth_gfp[start - pre_avg_smp : start])\n",
    "        \n",
    "        if average_near_GFP > average_futher_GFP:\n",
    "            continue\n",
    "    \n",
    "\n",
    "        #Part C:\n",
    "        #Check is there exits 25ms GFP decrease trend in 300hs before start to end time.\n",
    "        \n",
    "        #Consider the range is same with find lowest GFP, so just use the variable before\n",
    "        search_trend_data = smooth_gfp[start_check_point : end]\n",
    "        diffs = np.diff(search_trend_data)\n",
    "        \n",
    "        find_decrease_trend = False\n",
    "        is_decrease = (diffs <= 0).astype(int)\n",
    "        \n",
    "        #Find data decrease trend in 25ms\n",
    "        if np.max(np.convolve(is_decrease, np.ones(decrease_scope), mode='valid')) >= decrease_scope:\n",
    "            find_decrease_trend = True\n",
    "        \n",
    "        if find_decrease_trend:\n",
    "            verified_segments.append((start, end))\n",
    "    \n",
    "    return verified_segments\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15bfe63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function for merge all back to back task switch\n",
    "def merge_back_to_back(task_switch):\n",
    "    #Set variable\n",
    "    merged_segments = []\n",
    "    current_start, current_end = task_switch[0]\n",
    "    total = len(task_switch)\n",
    "    i=1\n",
    "    \n",
    "    while i < total:\n",
    "        if task_switch[i][0] <= current_end:\n",
    "            current_end = max(task_switch[i][1], current_end)\n",
    "            i+=1\n",
    "        else:\n",
    "            merged_segments.append((current_start, current_end))\n",
    "            current_start, current_end = task_switch[i]\n",
    "            i += 1\n",
    "    if merged_segments[len(merged_segments)-1][0] != current_start:\n",
    "        merged_segments.append((current_start, current_end))\n",
    "    \n",
    "    return merged_segments\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613c82a",
   "metadata": {},
   "source": [
    "Test data on function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23eea5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from data/sleep/SC4001E0-PSG.edf...\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 7949999  =      0.000 ... 79499.990 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/kdx178sn0hn6k06q5wb3d6km0000gn/T/ipykernel_6452/1136524986.py:3: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(file_path, preload=True)\n",
      "/var/folders/ms/kdx178sn0hn6k06q5wb3d6km0000gn/T/ipykernel_6452/1136524986.py:3: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(file_path, preload=True)\n",
      "/var/folders/ms/kdx178sn0hn6k06q5wb3d6km0000gn/T/ipykernel_6452/1136524986.py:3: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  raw = mne.io.read_raw_edf(file_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready, shape is : (7950000, 7)\n",
      "49991\n",
      "47673\n",
      "8686\n",
      "3780\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "file_path = \"data/sleep/SC4001E0-PSG.edf\"\n",
    "raw = mne.io.read_raw_edf(file_path, preload=True)\n",
    "\n",
    "data = raw.get_data()\n",
    "gfp = np.std(data, axis=0)\n",
    "    \n",
    "df_eeg = raw.to_data_frame(picks = 'eeg')\n",
    "if 'time' in df_eeg.columns:\n",
    "    df_eeg = df_eeg.drop(columns=['time'])\n",
    "    \n",
    "print(f\"Data is ready, shape is : {df_eeg.shape}\")\n",
    "\n",
    "try:\n",
    "    candidates = detect_task_switch_by_bhattacharyya_with_better_CPU(df_eeg, sfreq=100)\n",
    "    print(len(candidates))\n",
    "    smooth_gfp = calculate_GFP(df_eeg,sfreq=100)\n",
    "    \n",
    "    verified_at_segments = verify_alpha_theta_2(df_eeg, candidates, sfreq=100)\n",
    "    print(len(verified_at_segments))\n",
    "    final_segments = gfp_check(verified_at_segments, smooth_gfp, sfreq=100)\n",
    "    print(len(final_segments))\n",
    "    after_merge = merge_back_to_back(final_segments)\n",
    "    print(len(after_merge))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Errer: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
